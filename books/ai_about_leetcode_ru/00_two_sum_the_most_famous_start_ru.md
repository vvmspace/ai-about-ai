# 00. Two Sum: задача, на которой видно всё

Есть неудобная правда про алгоритмические интервью: иногда ваш уровень считывают по задаче, которую школьник решает за вечер. Именно так работает **Two Sum**. Если вы бросаетесь в код без модели, это видно сразу. Если вы спокойно формулируете инвариант и границы — это тоже видно.

Задача звучит почти несерьёзно: дан массив чисел и target, нужно вернуть индексы двух чисел, сумма которых даёт target. Кажется, что это «разминка». Но здесь интервьюер смотрит не на ответ, а на порядок мышления: как быстро вы отсекаете лишнее, как обосновываете сложность, как проверяете крайние случаи.

Самый прямой путь — двойной цикл за O(n²). Он рабочий, и это важно прямо озвучить. На интервью не требуется телепатия, требуется контроль:

> «Базовое решение есть: проверяю все пары, получаю O(n²) по времени и O(1) по памяти. Теперь улучшаю до O(n) через hash map».

Дальше включается теория, но в простом виде. Мы держим хеш-таблицу `value -> index`. На каждом шаге для текущего `x` считаем `need = target - x` и проверяем, встречали ли `need` раньше. Если да — ответ найден. Если нет — кладём `x` в таблицу и идём дальше. Это классический обмен памяти на время.

Инвариант тут короткий и мощный: *до позиции i в таблице лежат все элементы слева, и каждый из них доступен за O(1) в среднем*. Поэтому проверка пары делается мгновенно.

По сложности полезно проговорить оба варианта явно:
- brute force: **O(n²)** по времени и **O(1)** по памяти;
- hash map: **O(n)** по времени в среднем и **O(n)** по памяти.

На интервью это звучит взросло, когда вы добавляете: «В худшем случае у hash map возможны деградации из-за коллизий, но в реальной инженерной практике мы опираемся на *average-case O(1)* доступ». Если хотите говорить чуть сильнее, можно добавить термины *load factor* (плотность заполнения таблицы) и *rehashing* (перестройка таблицы при росте), потому что они напрямую влияют на стабильность доступа.

Ещё пара неочевидных, но полезных терминов:
- **Комплемент** — число `target - x`, которое «достраивает» текущий элемент до нужной суммы.
- **Асимптотика** — поведение алгоритма при росте `n`, без привязки к конкретному железу.
- **Амортизированная сложность** — средняя стоимость операции на длинной последовательности действий.

Пример на Python:

```python
def two_sum(nums, target):
    seen = {}  # Хеш-таблица: значение -> индекс.

    for i, x in enumerate(nums):
        need = target - x  # Комплемент: что нужно добавить к x, чтобы получить target.

        # Если комплемент уже встречался раньше, пара найдена.
        if need in seen:
            return [seen[need], i]

        # Сохраняем текущее значение и его индекс для следующих шагов.
        seen[x] = i

    # По условию LeetCode ответ обычно гарантирован, но для полноты вернём пустой массив.
    return []
```

Пример на JavaScript:

```javascript
function twoSum(nums, target) {
  const seen = new Map(); // Map даёт ожидаемый O(1) доступ в среднем.

  for (let i = 0; i < nums.length; i++) {
    const x = nums[i];
    const need = target - x; // Комплемент для текущего элемента.

    // Если комплемент уже был, возвращаем индексы пары.
    if (seen.has(need)) {
      return [seen.get(need), i];
    }

    // Иначе запоминаем текущее число и его индекс.
    seen.set(x, i);
  }

  // Защита на случай нестандартного входа.
  return [];
}
```

И коротко про сложность прямо в контексте кода:
- цикл идёт по массиву один раз, значит время **O(n)**;
- в `seen` в худшем случае попадут почти все элементы, значит память **O(n)**;
- с инженерной точки зрения важна не только Big-O, но и *latency profile* — насколько предсказуемо решение ведёт себя на реальных данных.

Что часто ломает кандидатов:
- кладут элемент в map до проверки и случайно используют один и тот же индекс дважды;
- путают «вернуть значения» и «вернуть индексы»;
- не проговаривают, что при дубликатах решение остаётся корректным.

Теперь интервью-ответ, который звучит зрелее, чем просто код:

> “I start with a brute-force baseline to make correctness explicit, then reduce time complexity using a hash map that stores previously seen values. Each step checks whether the complement is already available, so we get linear time with predictable memory trade-off.”  
> [«Я начинаю с brute-force базиса, чтобы явно зафиксировать корректность, затем снижаю сложность по времени через hash map с уже встреченными значениями. На каждом шаге проверяю комплемент, поэтому получаю линейное время с предсказуемым компромиссом по памяти».]

Пара фраз, которые стоит держать под рукой:

- “Let’s make the invariant explicit.”  
  [«Давайте явно зафиксируем инвариант».]
- “We trade memory for speed, deliberately.”  
  [«Мы осознанно меняем память на скорость».]
- “The baseline is slow, but it proves correctness.”  
  [«Базовое решение медленное, но доказывает корректность».]

Где это встречается в работе, а не только на LeetCode: дедупликация событий по ключу, быстрый поиск парных операций (refund ↔ charge), корреляция логов по request id, контроль конфликтов в потоках данных. Везде одна и та же идея: хранить уже увиденное в структуре с быстрым доступом.

Если хотите, это первая маленькая победа в книге: вы не просто «знаете Two Sum», вы умеете спокойно объяснить путь от наивного к оптимальному. На следующей главе про **Valid Parentheses** станет видно, как тот же уровень дисциплины переносится на стек и проверку корректности последовательностей.

Полезные ссылки:
- [Two Sum на LeetCode](https://leetcode.com/problems/two-sum/)
- [Оглавление книги](./README.md)
