# RAG без фантазий

RAG годами продавали как магию:

«**Подключите документы — получите идеальные ответы**».

В проде всё прозаичнее и полезнее.
RAG — это не магия.
Это retrieval engineering плюс governance.

Когда работает — кажется очевидным.
Когда ломается — ломается дорого и уверенно.

## Что RAG действительно решает

RAG помогает привязать ответы к источникам,
снижая долю неподкреплённых утверждений.

Он не гарантирует истину.
Он гарантирует шанс на обоснованный ответ,
если retrieval-контур здоров.

Разница критична.

## Где обычно ломается RAG

Типовой стек проблем:

- шумный корпус,
- неудачный chunking,
- слабые метаданные,
- нет политики свежести,
- нет обязательных citations,
- нет цикла оценки.

После этого обычно винят модель.
Но модель отвечает из того, что нашёл поиск.
Плохой retrieval = красиво оформленная ошибка.

## Сначала качество корпуса, потом эмбеддинги

Начинайте с corpus governance:

- определить источники истины,
- убрать дубликаты и устаревшие документы,
- классифицировать по чувствительности и владельцу,
- вести даты ревью.

Меньший, но чистый корпус обычно лучше огромного хаоса.

## Chunking — это архитектурное решение

Размер и границы chunk должны следовать смыслу документа:

- policy-доки: по секциям,
- техдоки: по функциям/модулям,
- meeting notes: по решениям/действиям.

Слепой фиксированный chunking часто рвёт смысловые границы.

## Метаданные — скрытый рычаг

Минимальные поля:

- source id,
- owner,
- created/updated,
- sensitivity,
- domain tags,
- verification status.

С хорошими метаданными фильтрация умная.
Без них результаты могут выглядеть релевантными, но быть операционно небезопасными.

## Retrieval-стратегия 2026

Сильные системы используют hybrid retrieval:

- keyword/BM25 для точных терминов,
- semantic search для смысловой близости,
- reranking для финальной точности.

Один режим редко устойчив для разных классов запросов.

## Citation-дисциплина обязательна

Если high-impact ответы выходят без ссылок на источники,
это импровизация, а не enterprise AI.

Требуйте:

- источник на каждый значимый тезис,
- confidence-маркировку при неполном evidence,
- явный режим «*доказательств недостаточно*».

Последний пункт напрямую сохраняет доверие.

## Полевой кейс: policy-ассистент

Компания запустила внутренний policy Q&A.
Пользователи жаловались: ответы уверенные, но иногда противоречат свежим обновлениям.

Причины:

- в индексе остались старые политики,
- не было freshness-фильтров,
- citations не были обязательными.

После исправлений:

- старые документы изолировали,
- добавили фильтр по актуальности,
- включили обязательные источники.

Доверие вернулось, потому что выводы стали проверяемыми.

## Антипаттерны

### 1) Embedding-first
Тюнят векторную модель, игнорируя гигиену корпуса.

### 2) Одноразовая валидация
Проверили при запуске и забыли.

### 3) Нет adversarial тестов
Система не тестируется на конфликтующие/пограничные запросы.

### 4) Скрытая логика retrieval
Пользователь не видит, почему выбран источник.

## Практика: аудит на 10 запросов

Для одного RAG-контура:

1. выберите 10 типовых вопросов,
2. просмотрите top retrieved chunks,
3. оцените релевантность и свежесть,
4. найдите один системный промах,
5. внесите одно изменение в retrieval/metadata,
6. повторите те же 10 запросов.

Делайте это еженедельно.

Маленькие аудиты ловят дрейф до потери доверия.

## RAG как продукт, а не фича

У RAG должны быть SLA-метрики:

- покрытие citations,
- groundedness,
- соблюдение свежести,
- честность fallback-ответов.

Если это не измеряется, это нельзя безопасно эксплуатировать.

## Ключевая мысль

RAG великолепно работает при дисциплинированной работе с данными.
И громко ломается, когда его воспринимают как plug-and-play магию.

Фраза, которую стоит держать рядом:

**Качество retrieval — это качество истины на входе.**
